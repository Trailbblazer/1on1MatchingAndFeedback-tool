# Test Set Information

The two provided test datasets, **testSet1** and **testSet2**, are designed to validate the correctness of the algorithm after any modifications or updates you may have made. By following the steps below, you can ensure that the algorithm functions as intended.

## Purpose
These test datasets serve as a benchmark to verify that the algorithm produces accurate and expected results. They contain predefined input files and expected outputs, allowing for a straightforward comparison between the actual and desired outcomes.

## How to Use the Test Sets
1. **Prepare Input Files:**
   - Copy the values from the following test files into their respective input files:
     - `test_coachTimeWithBreaks.json` → `coachTimeWithBreaks.json`
     - `test_startups.json` → `startups.json`
     - `test_total_feedbacks.json` → `total_feedbacks.json`

2. **Run the Algorithm:**
   - Execute the `algo.py` script.

3. **Validate the Output:**
   - Compare the output generated by the algorithm to the expected results:
     - The output in `algo_output.md` should match the actual results.
     - The values in `assigned_startups.json` should match those in `expected_assigned_startups.json`.
     - The `startups.json` file should reflect updated meeting counts identical to those in `expected_startups.json`.

## Success Criteria
The algorithm is considered to be functioning correctly if all of the following conditions are met:
- The contents of `algo_output.md` accurately describe the output of the algorithm.
- The `assigned_startups.json` file matches `expected_assigned_startups.json`.
- The meeting counts in `startups.json` are updated to match `expected_startups.json`.

## Notes
- Ensure that all input and output files are correctly updated before running the algorithm.
- If discrepancies are found, investigate the algorithm's logic and address any issues before re-testing.

By adhering to these steps, you can confidently verify the accuracy and reliability of the algorithm after any changes.

